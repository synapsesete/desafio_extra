import os

from typing import Type

from dotenv import load_dotenv
load_dotenv()

from typing import List, Any

from langchain.agents import AgentExecutor, create_react_agent
from langchain_core.runnables.base import Runnable
from langchain_experimental.tools.python.tool import PythonREPLTool
from langchain.memory import ConversationBufferMemory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.callbacks import BaseCallbackHandler
from langchain.agents.react.output_parser import ReActOutputParser

from langchain.callbacks import StreamlitCallbackHandler


class AgenteAnaliseDadosCSV:

    def __init__(self,chat_memory: Type[BaseChatMessageHistory],callbacks: List[BaseCallbackHandler]):
        """
        Inicializar o Agente de análise de dados passando para ele o texto de prompt do que deve ser feito.
        """
        self.__calbacks = callbacks
        tools = self._set_toolkit()
        llm = self._load_llm()
        llm_with_tools = llm.bind_tools(tools)
        prompt = self._load_prompt()

        from parsers import CustomAgentOutputParser

        agent = create_react_agent(
            llm=llm_with_tools,
            tools=tools,
            prompt=prompt,
            output_parser=CustomAgentOutputParser(),
        )

        self.__agent_executor = AgentExecutor(
            agent=agent, 
            tools=tools, 
            handle_parsing_errors=True, 
            verbose=True, 
            memory=ConversationBufferMemory(chat_memory=chat_memory, memory_key="chat_history", return_messages=True)
        )

    def _set_toolkit(self) -> List[str]:
        tools = [
            PythonREPLTool()
        ]
        return tools

    def _load_llm(self) -> Runnable:
        from langchain_google_genai import ChatGoogleGenerativeAI
        from langchain_ollama import ChatOllama

        if os.environ.get("GOOGLE_API_KEY"):
            llm = ChatGoogleGenerativeAI(model=os.environ["LLM_MODEL"], temperature=0)
        else:
            llm = ChatOllama(
                temperature=0,
                model=os.environ["OLLAMA_LLM_MODEL"],
                base_url=os.environ["OLLAMA_URL"],
            )

        return llm

    def _load_prompt(self):
        from langchain import hub

        prompt = hub.pull("langchain-ai/react-agent-template")
        prompt = prompt.partial(instructions=self._load_instructions())

        return prompt

    def _load_instructions(self) -> str:

        prompt_path = os.environ['INSTRUCOES_PATH']
        with open(prompt_path, "r", encoding="utf-8") as f:
            custom_prompt = f.read()

        print("Instruções do agente carregada com sucesso!")

        return custom_prompt

    def invoke(self, question: str) -> Any:
        return self.__agent_executor.invoke({"input": question},callbacks=self.__calbacks)['output']
    